Branch-Aware Workflow Scheduler for WSI Analysis

A concurrency, multi-tenant workflow scheduler designed for processing Gigapixel Whole Slide Images (WSI).

This system implements a Branch-Aware DAG Scheduler that enforces serial execution within branches and parallel execution across branches, bounded by global worker limits and per-user concurrency quotas. It features a Real AI Inference Engine integrating InstanSeg and OpenSlide to perform cell segmentation.


üöÄ Key Features

Multi-Tenant Isolation: Strict data isolation per X-User-ID.

Concurrency Control:

Active User Slots: Limits system to 3 concurrent active users. The 4th user is queued until a slot opens.

Global Workers: Limits total concurrent jobs to 4.

Resource Reclaiming: Implements "Hard Kill" logic to immediately release worker slots upon job cancellation.

Branch-Aware Scheduling:

Intra-branch: Serial execution (FIFO).

Inter-branch: Parallel execution.

Fail-Fast: If a job fails/cancels, all subsequent pending jobs in that branch are auto-cancelled.



üõ†Ô∏è Architecture

The system is containerized using Docker Compose, consisting of:

Scheduler API (FastAPI): Handles DAG creation, job dispatching, and state management.

Worker Engine: Integrated asyncio workers running Deep Learning tasks (InstanSeg/PyTorch) and WSI processing (OpenSlide).

Database (PostgreSQL): Persistent storage for workflow states and job metadata.


üèÅ Quick Start

Prerequisites

Docker & Docker Compose installed.

(Optional) A WSI file (.svs) for testing real pathological data.

1. Prepare Data (Important!)

To demonstrate the GB-level WSI processing capability, please download a sample Aperio SVS file.

Download CMU-1.svs (~170MB) from the OpenSlide Test Data repository:

Download Link

Place the file in the data/ directory at the project root:

# Your project structure should look like this:
/branch-workflow-scheduler
  ‚îú‚îÄ‚îÄ docker-compose.yml
  ‚îú‚îÄ‚îÄ app/
  ‚îî‚îÄ‚îÄ data/
      ‚îî‚îÄ‚îÄ CMU-1.svs  <-- Place file here


Note: If CMU-1.svs is not available, the system's SmartSlide adapter will gracefully fallback to processing standard images (e.g., data/test.png) using the same tiling logic.


2. Build & Run

Launch the full stack (Database + API + AI Engine):

docker-compose up --build


Wait until you see: Uvicorn running on http://0.0.0.0:8000.


3. Access the Dashboard

Open your browser and navigate to:
üëâ http://localhost:8000/dashboard

Create a Workflow: Click "+ New Workflow".

Run Demo: Select both "Core Analysis" and "Visualization" branches. The system will automatically detect data/CMU-1.svs and begin tiled inference.

Multi-User Test: Change the "User Context" input (e.g., to User2) to simulate multi-tenant queuing.


üìñ API Documentation (OpenAPI/Swagger)

Comprehensive API documentation is auto-generated by FastAPI.
Once the server is running, access:

üëâ http://localhost:8000/docs

Key Endpoints

POST /api/workflows: Create a new DAG.

GET /api/workflows/{id}: Get real-time status (polled by Dashboard).

POST /api/jobs/{id}/cancel: Cancel a running job (triggers Fail-Fast & Resource Reclaim).


üìä Scaling Strategy (10x - 100x)

Currently, the system uses asyncio.Task for execution and PostgreSQL for storage. To scale to 10x load:

Decouple Workers (Queue-Based):

Replace internal asyncio workers with a distributed task queue like Celery or ARQ backed by Redis.

This allows the API tier (I/O bound) and Worker tier (CPU/GPU bound) to scale independently.

Distributed State:

Move the in-memory _active_users slot tracking to Redis Atomic Counters.

Use Redis Distributed Locks (Redlock) to enforce branch serialization across multiple Scheduler replicas.

Storage:

Migrate local outputs/ storage to Object Storage (AWS S3 or MinIO).

Use Pre-signed URLs for frontend visualization.

Database:

Implement Read Replicas for the GET /status polling traffic.


